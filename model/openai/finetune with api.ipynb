{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17548,"status":"ok","timestamp":1708328144100,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"8j-0FQqeXT0b","outputId":"8bf90a40-1840-450d-dedf-55fc73cd7b77"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'openai-cookbook'...\n","remote: Enumerating objects: 5265, done.\u001b[K\n","remote: Counting objects: 100% (1059/1059), done.\u001b[K\n","remote: Compressing objects: 100% (765/765), done.\u001b[K\n","remote: Total 5265 (delta 308), reused 1037 (delta 294), pack-reused 4206\u001b[K\n","Receiving objects: 100% (5265/5265), 240.73 MiB | 18.41 MiB/s, done.\n","Resolving deltas: 100% (2809/2809), done.\n","Updating files: 100% (1274/1274), done.\n"]}],"source":["# 複製openai使用教學\n","!git clone https://github.com/openai/openai-cookbook.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32244,"status":"ok","timestamp":1708329031341,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"egh99ctRT1VK","outputId":"292e3b74-f29b-4e50-e4b4-93932538bc9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openai==1.6.1\n","  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/225.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/225.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.6.1) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai==1.6.1)\n","  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1) (2.6.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1) (1.3.0)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1) (4.9.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.6.1) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.6.1) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.6.1) (2024.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.6.1)\n","  Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.6.1)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.6.1) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.6.1) (2.16.2)\n","Installing collected packages: h11, httpcore, httpx, openai\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.3 httpx-0.26.0 openai-1.6.1\n","Collecting tiktoken\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Installing collected packages: tiktoken\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tiktoken-0.6.0\n","Collecting datasets\n","  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Collecting pyarrow>=12.0.0 (from datasets)\n","  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 10.0.1\n","    Uninstalling pyarrow-10.0.1:\n","      Successfully uninstalled pyarrow-10.0.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.17.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-15.0.0\n"]}],"source":["# 下載最新版的openai套件\n","!pip install openai==1.6.1  # origin using 1.12\n","# 下載token計算套件\n","!pip install tiktoken\n","!pip install datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2583,"status":"ok","timestamp":1708329162061,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"-KCqzNlWvQg6"},"outputs":[],"source":["import openai\n","import json\n","# for token counting\n","import tiktoken\n","import numpy as np\n","from collections import defaultdict\n","from datasets import load_dataset\n","# drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1708329931716,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"BaBdPYlBwM_J","outputId":"b497afed-53b3-48ae-cab7-793d9c80aae2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Num examples: 1783\n","First example:\n","{'role': 'system', 'content': '法條索引'}\n","{'role': 'user', 'content': '公司法第 340 條?'}\n","{'role': 'assistant', 'content': '公司對於其債務之清償，應依其債權額比例為之。但依法得行使優先受償權或別除權之債權，不在此限。 '}\n"]}],"source":["data_path = \"\"  #/toy_chat_fine_tuning.jsonl\n","\n","# Load the dataset\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    dataset = [json.loads(line) for line in f]\n","\n","# Initial dataset stats\n","print(\"Num examples:\", len(dataset))\n","print(\"First example:\")\n","for message in dataset[0][\"messages\"]:\n","    print(message)\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708329934597,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"uh293sn_yTlQ","outputId":"c0bfd088-0f02-48cf-9642-9d64d599919b"},"outputs":[{"name":"stdout","output_type":"stream","text":["No errors found\n"]}],"source":["# 檢查資料格式是否符合openai官方設定\n","format_errors = defaultdict(int)\n","\n","for ex in dataset:\n","    if not isinstance(ex, dict):\n","        format_errors[\"data_type\"] += 1\n","        continue\n","\n","    messages = ex.get(\"messages\", None)\n","    if not messages:\n","        format_errors[\"missing_messages_list\"] += 1\n","        continue\n","\n","    for message in messages:\n","        if \"role\" not in message or \"content\" not in message:\n","            format_errors[\"message_missing_key\"] += 1\n","\n","        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n","            format_errors[\"message_unrecognized_key\"] += 1\n","\n","        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n","            format_errors[\"unrecognized_role\"] += 1\n","\n","        content = message.get(\"content\", None)\n","        function_call = message.get(\"function_call\", None)\n","\n","        if (not content and not function_call) or not isinstance(content, str):\n","            format_errors[\"missing_content\"] += 1\n","\n","    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n","        format_errors[\"example_missing_assistant_message\"] += 1\n","\n","if format_errors:\n","    print(\"Found errors:\")\n","    for k, v in format_errors.items():\n","        print(f\"{k}: {v}\")\n","else:\n","    print(\"No errors found\")"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2157,"status":"ok","timestamp":1708329945903,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"2SgMumGfzMjM"},"outputs":[],"source":["# 數訓練資料中中包含幾個token\n","encoding = tiktoken.get_encoding(\"cl100k_base\")  #tiktoken is a fast BPE tokeniser for use with OpenAI's models.\n","\n","# not exact!\n","# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n","def num_tokens_from_messages(instruction, tokens_per_message=3, tokens_per_name=1):\n","    num_tokens = 0\n","    for message in messages:\n","        num_tokens += tokens_per_message\n","        for key, value in message.items():\n","            num_tokens += len(encoding.encode(value))\n","            if key == \"name\":\n","                num_tokens += tokens_per_name\n","    num_tokens += 3\n","    return num_tokens\n","\n","def num_assistant_tokens_from_messages(messages):\n","    num_tokens = 0\n","    for message in messages:\n","        if message[\"role\"] == \"assistant\":\n","            num_tokens += len(encoding.encode(message[\"content\"]))\n","    return num_tokens\n","\n","def print_distribution(values, name):\n","    print(f\"\\n#### Distribution of {name}:\")\n","    print(f\"min / max: {min(values)}, {max(values)}\")\n","    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n","    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1708329962298,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"k2jIwNyNUIw7","outputId":"f63a41bc-d62b-4525-a880-d316e60b659f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset has ~5349 tokens that will be charged for during training\n","By default, you'll train for 3 epochs on this dataset\n","By default, you'll be charged for ~16047 tokens\n"]}],"source":["# 預計訓練次數和價格\n","MAX_TOKENS_PER_EXAMPLE = 4096\n","\n","TARGET_EPOCHS = 3\n","MIN_TARGET_EXAMPLES = 100\n","MAX_TARGET_EXAMPLES = 25000\n","MIN_DEFAULT_EPOCHS = 1\n","MAX_DEFAULT_EPOCHS = 25\n","\n","n_epochs = TARGET_EPOCHS\n","n_train_examples = len(dataset)\n","if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n","    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n","elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n","    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n","\n","convo_lens = [len(conversation['messages']) for conversation in dataset]\n","n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n","print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n","print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n","print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_gAl86dbrZk"},"outputs":[],"source":["training_file = \"\"\n","validation_file = \"\""]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2250,"status":"ok","timestamp":1708330011531,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"6ehkrpKiMazZ","outputId":"72b5a072-5be2-4961-bb23-0003e47c649c"},"outputs":[{"data":{"text/plain":["FileObject(id='file-yI9mGxdfZnFY8IK6avk0z8kX', bytes=814479, created_at=1708330010, filename='remain_lawdata.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# 上傳訓練資料\n","\n","\n","from openai import OpenAI  #for verion 1.12\n","client = OpenAI(api_key=\"\")\n","\n","client.files.create(\n","  file=open(\"\" , \"rb\"),\n","  purpose=\"fine-tune\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1214,"status":"ok","timestamp":1708017588811,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"iTbTdQbBOTKp","outputId":"81b4463d-2216-4f53-82fe-3867710f7d1c"},"outputs":[{"data":{"text/plain":["FileObject(id='file-ZVCGTcNrMzYDBHVYyfBc1PlB', bytes=238352, created_at=1708017588, filename='test-api.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["from openai import OpenAI\n","client = OpenAI(api_key=\"\")\n","\n","client.files.create(\n","  file=open(validation_file, \"rb\"),\n","  purpose=\"fine-tune\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"elapsed":11,"status":"error","timestamp":1708011940014,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"SeWC330POujB","outputId":"64f3811d-643b-4846-b6c1-9499ef70a122"},"outputs":[{"ename":"AttributeError","evalue":"type object 'OpenAI' has no attribute 'file'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-aaff661ca260>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOpenAI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file-zNnRhfyCXhtSq1lVtRVVylx6\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#trace the file status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: type object 'OpenAI' has no attribute 'file'"]}],"source":["# OpenAI.file.retrieve(\"file-zNnRhfyCXhtSq1lVtRVVylx6\")  #trace the file status"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1708330902535,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"29X2aqBc_77Q","outputId":"b6aaa456-0e64-4351-e486-e3a4d7f00cba"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m10622130\u001b[0m (\u001b[33mtw-nlp-law\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# !pip install wandb  #sync the traning data with wandb\n","import wandb\n","wandb.login()\n","# !pip install requests"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":710,"status":"error","timestamp":1708330972332,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"nqJo0QC7SiDM","outputId":"d71c0e04-ac62-4ab3-b653-1afe64f7398f"},"outputs":[{"ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': 'invalid training_file: /content/remain_lawdata.jsonl', 'type': 'invalid_request_error', 'param': 'training_file', 'code': None}}","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-93351a628b3b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"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\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m response = client.fine_tuning.jobs.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mtraining_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/remain_lawdata.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/fine_tuning/jobs.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, training_file, hyperparameters, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;34m\"/fine_tuning/jobs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m         )\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 853\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return self._process_response(\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'invalid training_file: /content/remain_lawdata.jsonl', 'type': 'invalid_request_error', 'param': 'training_file', 'code': None}}"]}],"source":["# 建立並執行finetune任務\n","from openai import OpenAI\n","from openai.types.fine_tuning import FineTuningJob, FineTuningJobEvent\n","client = OpenAI(api_key=\"\")\n","\n","response = client.fine_tuning.jobs.create(\n","  training_file=\"\",\n","  model=\"gpt-3.5-turbo\",\n","  suffix=\"lawdata01\"\n","  # hyperparameters={\n","  #   \"n_epochs\":2\n","  # }  # the default epoch is 3\n",")\n","\n","job_id = response.id\n","print(\"Job ID:\", response.id)\n","print(\"Status:\", response.status)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"elapsed":175004,"status":"ok","timestamp":1708332942461,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"2-DskzfIQPEr","outputId":"b4aaaf69-ac4f-4df6-f0c8-a29f1f7158a9"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Retrieving fine-tune job...\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20240219_085248-ftjob-3hu0ac8xCD7YXLr0yeoSkH5Z</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/tw-nlp-law/OpenAI-Fine-Tune/runs/ftjob-3hu0ac8xCD7YXLr0yeoSkH5Z' target=\"_blank\">ftjob-3hu0ac8xCD7YXLr0yeoSkH5Z</a></strong> to <a href='https://wandb.ai/tw-nlp-law/OpenAI-Fine-Tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/tw-nlp-law/OpenAI-Fine-Tune' target=\"_blank\">https://wandb.ai/tw-nlp-law/OpenAI-Fine-Tune</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/tw-nlp-law/OpenAI-Fine-Tune/runs/ftjob-3hu0ac8xCD7YXLr0yeoSkH5Z' target=\"_blank\">https://wandb.ai/tw-nlp-law/OpenAI-Fine-Tune/runs/ftjob-3hu0ac8xCD7YXLr0yeoSkH5Z</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for the OpenAI fine-tuning job to be finished...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fine-tune ftjob-3hu0ac8xCD7YXLr0yeoSkH5Z has failed and will not be logged\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fine-tune ftjob-3hu0ac8xCD7YXLr0yeoSkH5Z has the status \"failed\" and will not be logged\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'🎉 wandb sync completed successfully'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","from wandb.integration.openai.fine_tuning import WandbLogger  #sync the data\n","\n","# Finetuning logic\n","os.environ[\"OPENAI_API_KEY\"] =\"\"\n","\n","\n","WandbLogger.sync(fine_tune_job_id=\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":966,"status":"ok","timestamp":1704179915021,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"fIDIyAQpUAt6","outputId":"cd19caeb-94ea-4e7c-b3f6-9247d7b53904"},"outputs":[{"data":{"text/plain":["SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-lq3HgqDGrQvkao3gyfKzQFQe', created_at=1704179766, level='info', message='Fine-tuning job started', object='fine_tuning.job.event', data=None, type='message'), FineTuningJobEvent(id='ftevent-v0EdVu47HiNR9Y8Qc1bgGhS6', created_at=1704179765, level='info', message='Files validated, moving job to queued state', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-4IH9jeWqJf98My2R3NZopGo0', created_at=1704179764, level='warn', message='File file-0w4HebAfYP54nBaExzTaLORe contains examples greater than the supported context size for model `gpt-3.5-turbo-0613` (4096 tokens)', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-qLnczeTqrohvRKTuqMpHgAG7', created_at=1704179763, level='info', message='Validating training file: file-0w4HebAfYP54nBaExzTaLORe', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-CCbyvTv3dOYhhvYOIX2GRSCr', created_at=1704179763, level='info', message='Created fine-tuning job: ftjob-lhVYdXMCkMKIODq1GjAzThze', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["# 此區塊用於對訓練任務進行操作\n","from openai import OpenAI\n","client = OpenAI(api_key=\"\")\n","\n","# 列出fine tune 任務\n","client.fine_tuning.jobs.list(limit=10)\n","\n","# 檢索fine tune 狀態\n","client.fine_tuning.jobs.retrieve(\"\")\n","\n","# 取消finetune\n","# client.fine_tuning.jobs.cancel(\"ftjob-lhVYdXMCkMKIODq1GjAzThze\")\n","\n","# List up to 10 events from a fine-tuning job\n","client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"\", limit=10)\n","\n","# Delete a fine-tuned model (must be an owner of the org the model was created in)\n","# client.models.delete(\"ft:gpt-3.5-turbo:acemeco:suffix:test1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18555,"status":"ok","timestamp":1708059790650,"user":{"displayName":"Shuo Bing Jhang","userId":"16535721502250842542"},"user_tz":-480},"id":"qm-P0PH_--Pk","outputId":"af323cb0-8a4c-4dfd-e0bb-78bc8ecc3d50"},"outputs":[{"name":"stdout","output_type":"stream","text":["車禍該怎麼辦依照所造成的損害（動物財物車輛傷亡等），請洽詢各級法院簡易事件調解室程序依程序依以下步驟進行：案件的受理：由申請人檢具名冊照片、原車照片（全車照片、傷害部位各張）、被申請人之駕照複印本或事實稱說之相關證據，及iType追車單（必要時）、交通事故原因分析書（必要時）；提交調解申請書；本院按日受理調解案件；不具備前揭條款規定資格之申請書，本院得要求申請人補正之。本院另不予進行案件受理之之人身傷害給付說明書或其他機關調解同一之請求。不予複受理。日曆調解期日之確定：本院自受理之次日起，會同申請人、被申請人，先期約定日曆調解談判期日一次。談判期日確定後，遇到調解員突發因素（例：受傷急難醫治、家庭緊急事故、天災）等受不可抗力因素，或其他申請人、被申請人遇調解員約定日曆調解時間先前無原因通知本院復日曆調解，致本院聲請元日曆調解之通知失效，致當日未能依期抵達調解地點者，其所受理之民事日曆調解案件，第一次約定調解期日不予補約，除其他社會情緒上更有幫助調解結果與目的者外，在依任何一方之請求，或經本院之求等，得適當裁定讓請求之一方指定日曆調解期日；若已符合天然人民公正訴訟法第56條之要件者予以獄同第三項前段之處分。原則上檢附資料越清楚越有助為你們分辨糾紛之結果。合議調解因數有理事實埋怨民眾及各式爭執械受理昆明市當鋪被告陳則燊之高等法院暨其訴訟及非訟代理人除中華民國律師或訴訟代理人外，不得參與案件執行資料なりわれわれは、自分たちはまだ自分の正当な権利を維持する是非に反省して、対立を生む可能性がある場所に入ることがあります。私たちの対話にはいったん、私たちは新しい出会いをどこまでも暖かく精力的にディテクシヨンのディテクター、トップ、Rulesへのアクセスが必要です.\n"]}],"source":["# 使用模型\n","from openai import OpenAI\n","\n","client = OpenAI(organization = '', api_key='')\n","\n","response = client.chat.completions.create(\n","  model=\"\",\n","  messages=[\n","    {\"role\": \"system\", \"content\": \"你是一位對每個問題給予客觀建議的專業法律顧問。\"},\n","    {\"role\": \"user\", \"content\": \"車禍該怎麼辦\"},\n","  ]\n",")\n","\n","print((response.choices[0].message.content))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4I-4FblT2JU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
